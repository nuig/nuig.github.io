#! /usr/bin/env python

'''
@author Marcos Cardinot

Python script to update the list of publications 'autogen_all.bib'
Strategy:
 - collect sources from DBLP for each staff author (store them)
 - merge all files removing duplicates
 - check for exclusion list
 - overwrite/add ids from 'manual.bib'

'''

#####################################################################
# SETTINGS
#####################################################################

# url to the bibtex files at https://dblp.uni-trier.de
dblp = [
  {
    "name": "colm",
    "url": 'https://dblp.uni-trier.de/pers/tb1/o/O=Riordan:Colm.bib'
  },
  {
    "name": "josephine",
    "url": 'https://dblp.uni-trier.de/pers/tb1/g/Griffith:Josephine.bib'
  },
  {
    "name": "seamus",
    "url": 'https://dblp.uni-trier.de/pers/tb1/h/Hill:Seamus.bib'
  }
]

# ids to exclude from the final database
black_list = []

# bibliography directory
bibliography_dir = "_bibliography/"

# filename for the entries added manually
manual_filename = 'custom_entries.bib'

#####################################################################
# DO NOT CHANGE THE SCRIPT BELOW
# unless you know what you're doing =D
#####################################################################

import urllib2
import bibtexparser
from bibtexparser.bparser import BibTexParser
from bibtexparser.customization import *
from datetime import datetime
import os

# return date string in YYYY-MM-DD format
def parseDate(date_text):
    try:
        date_text = date_text.split(' +')[0]
        dt = datetime.strptime(date_text, '%a, %d %b %Y %H:%M:%S')
        return dt.strftime("%Y-%m-%d")
    except ValueError:
        dt = datetime.strptime(date_text, '%Y-%m-%d')
        return date_text

def fix_format(record):
    if record['timestamp']:
        record['timestamp'] = parseDate(record['timestamp'])
    if record['ID']:
        record['ID'] = record['ID'].replace("DBLP:", "", 1)
        record['ID'] = record['ID'].replace("/", "_")
    return record

def loadBibtex(bibstr):
    parser = BibTexParser()
    parser.customization = fix_format
    return bibtexparser.load(bibstr, parser=parser)

def merge(bib1, bib2, check_black_list=True):
    merged = bib1

    entrylist1 = bib1.get_entry_list()
    keys1 = set([entry['ID'] for entry in entrylist1])

    entrylist2 = bib2.get_entry_list()
    keys2 = set([entry['ID'] for entry in entrylist2])

    duplicates = keys1 & keys2
    uniquekeys = keys1.union(keys2)
    print('  -> merging %d + %d entries (removes %d duplicates)'
            % (len(entrylist1), len(entrylist2), len(duplicates)))

    for entry in entrylist2:
        if check_black_list and entry['ID'] in black_list:
            print('  -> removing %s' % entry['ID'])
        elif entry['ID'] not in duplicates:
            merged.entries.append(entry)

    return merged

def mergePreprints(bib):
    # take the preprints from the database
    bibPreprints = bibtexparser.bibdatabase.BibDatabase()
    newbib = bibtexparser.bibdatabase.BibDatabase()
    for entry in bib.get_entry_list():
        if 'archiveprefix' in entry and entry['archiveprefix'].lower() == 'arxiv':
            bibPreprints.entries.append(entry)
        else:
            newbib.entries.append(entry)

    if len(bibPreprints.entries) == 0:
        return newbib

    # for each publication, check if it matches with a preprint
    for published in newbib.get_entry_list():
        match = False
        for preprint in bibPreprints.get_entry_list():
            if published['title'] == preprint['title'] and published['author'] == preprint['author']:
                match = True
                break

        if not match:
            continue

        print('  -> merging preprint \'%s\' with \'%s\'' % (preprint['ID'], published['ID']))
        published['arxiv'] = preprint['url']
        bibPreprints.entries.remove(preprint)

    # keep the preprints without a match; fix the type to 'unpublished'
    for preprint in bibPreprints.get_entry_list():
        print('  -> keeping preprint \'%s\'' % (preprint['ID']))
        preprint['ENTRYTYPE'] = 'unpublished'
        preprint['arxiv'] = preprint['eprint']
        preprint['timestamp'] = datetime.now().strftime("%Y-%m-%d") # move to the top
        newbib.entries.append(preprint)

    return newbib

def dblpDatabase():
    merged = None
    uniquekeys = set()
    for staff in dblp:
        bibstr = urllib2.urlopen(staff["url"])
        bib = loadBibtex(bibstr)
        print('DBLP:%s' % staff["name"])

        if merged is None:
            merged = bib
            print('  -> reading %d entries' % len(bib.entries))
        else:
            merged = merge(merged, bib)

        with open('autogen_dblp_%s.bib' % staff["name"], 'w') as bibfile:
            bibtexparser.dump(bib, bibfile)

    print('Total from DBLP: %d entries\n' % len(merged.get_entry_list()))
    return merged

def main():
    if not os.path.exists(bibliography_dir):
        print("ERROR! The bibliography directory was not found!")
        return
    elif not os.path.exists(bibliography_dir + manual_filename):
        print("ERROR! The file with the manual entries was not found!")
        return

    os.chdir(bibliography_dir)
    database = dblpDatabase()

    with open(manual_filename, 'r') as bibstr:
        bib = loadBibtex(bibstr)
        entries = len(bib.entries)
        if entries > 0:
            print('Manual entries\n  -> reading %d entries' % entries)
            database = merge(bib, database)

    database = mergePreprints(database)

    filename = 'autogen_all.bib'
    with open(filename, 'w') as bibfile:
        bibtexparser.dump(database, bibfile)

    print('\nFinished!\n%d entries exported to \'%s%s\''
            % (len(database.get_entry_list()), bibliography_dir, filename))

main()
